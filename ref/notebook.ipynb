{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Data under the hood\n",
    "\n",
    "This notebook will provide a high-level overview of the architecture of the `Ray Data` library.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Here is the Roadmap:</b>\n",
    "    <ul>\n",
    "        <li>Streaming execution</li>\n",
    "        <li>Dataset and blocks</li>\n",
    "        <li>Ray memory model</li>\n",
    "        <li>Operators and planning</li>\n",
    "        <li>Streaming topology</li>\n",
    "        <li>Data flow within an operator</li>\n",
    "        <li>Streaming executor's scheduling loop</li>\n",
    "        <li>Resource management and allocation</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import ray\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming execution\n",
    "\n",
    "Ray Data processes large datasets efficiently using a streaming model, which works with **blocks** as the basic units of data.\n",
    "\n",
    "This approach replaces traditional bulk processing, where the entire dataset and intermediate results had to fit in the cluster's memory.\n",
    "\n",
    "For example:\n",
    "\n",
    "Here is a batch inference pipeline with a bulk processing approach.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/batch-processing.png\" width=\"800\" alt=\"Traditional Batch Processing\">\n",
    "\n",
    "Note how:\n",
    "- Execution is performed in stages\n",
    "- The entire dataset can be repartitioned across stage boundaries\n",
    "\n",
    "In contrast, here is the same batch inference pipeline with Ray Data's streaming model.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/pipelining.png\" width=\"800\" alt=\"Streaming Model Pipelining\">\n",
    "\n",
    "Note how:\n",
    "- Execution across stages of the pipeline is performed in parallel (i.e pipeline parallelism)\n",
    "- Data is passed incrementally without blocking the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\">\n",
    "    \n",
    "Ray Data was first implemented using a bulk processing approach (now referred to as \"legacy API\").\n",
    "\n",
    "Some artifacts of the legacy API, like `Dataset.repartition`, still reflect the older bulk processing model, which required the entire dataset to be partitioned across the cluster.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and blocks\n",
    "\n",
    "A Ray Dataset defines a data loading and processing pipeline.\n",
    "\n",
    "When either \"materialized\" or \"consumed\", a Ray Dataset manifests as a distributed collection of blocks stored in the Ray Object Store.\n",
    "\n",
    "Let's start by creating a materialized Ray Dataset to inspect its underlying blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: List the existing objects before executing the code\n",
    "\n",
    "Running the below command will list the objects in the Ray object store.\n",
    "\n",
    "The Ray object store is a shared memory space to store and pass data between tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there are no objects created yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare some data\n",
    "\n",
    "Let's build a parquet dataset given a target in-memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_mb = 64\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"a\": np.random.rand(size_mb * 1024**2 // 8).astype(np.float64),\n",
    "    }\n",
    ")\n",
    "\n",
    "memory_usage = (df.memory_usage(deep=True) / 1024**2).sum() # in MiB\n",
    "print(f\"Memory usage: {memory_usage} MiB\")\n",
    "\n",
    "df.to_parquet(\"/mnt/cluster_storage/data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the parquet file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /mnt/cluster_storage/data.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a materialized dataset\n",
    "\n",
    "Let's create a `Dataset` from the parquet file using `read_parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.read_parquet(\"/mnt/cluster_storage/data.parquet\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's materialize the `Dataset` using `materialize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_materialized = ds.materialize()\n",
    "ds_materialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: List the objects again\n",
    "\n",
    "We can see an object with a size of ~64 MiB has been created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can verify the object was indeed generated by a Ray Data task by following the `CALL_SITE` of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Inspect the blocks\n",
    "\n",
    "Instead of browsing through all the objects in the object store, we can directly fetch the blocks of a materialized dataset using Ray Data.\n",
    "\n",
    "It turns out that we can iterate over the blocks of a dataset using `iter_internal_ref_bundles`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref_bundle in ds_materialized.iter_internal_ref_bundles():\n",
    "    print(ref_bundle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reference bundle `RefBundle` is simply a bundle of:\n",
    "- a reference to the block\n",
    "- metadata about the block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_ref, block_metadata = ref_bundle.blocks[0]\n",
    "block_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A block is the basic unit of data that Ray Data stores in the object store and transfers over the network. \n",
    "\n",
    "Each block contains a disjoint subset of rows, and Ray Data loads and transforms these blocks in a distributed manner.\n",
    "\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/dataset-arch.svg\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch the block, we can use `ray.get`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = ray.get(block_ref)\n",
    "block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Data stores blocks as either pandas Dataframes or pyarrow Tables. In this case, when materializing from a `read_parquet`, the block is a pyarrow Table.\n",
    "\n",
    "<!-- TODO - figure out adding info below: -->\n",
    "<!-- Note, that regardless of the data type that Ray Data uses to store the block, Ray Data will convert the block to the required batch format when batching the data and transforming it. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the block contains the same data as the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block.shape, df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's clean up references to the objects we created so Ray can garbage collect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xdel block\n",
    "%xdel block_ref\n",
    "%xdel ds\n",
    "%xdel ds_materialized\n",
    "%xdel ref_bundle\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the object has been garbage collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the number of blocks\n",
    "<!-- Disclaimer that users ideally should not have to worry about this -->\n",
    "\n",
    "#### Block size limiting\n",
    "Ray Data bounds block sizes to avoid excessive communication overhead and prevent out-of-memory errors.\n",
    "\n",
    "* Small blocks are good for latency and more streamed execution\n",
    "* Large blocks reduce scheduler and communication overhead. \n",
    "\n",
    "The default range attempts to make a good tradeoff for most jobs. \n",
    "\n",
    "Here are the default values that Ray Data uses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = ray.data.DataContext.get_current()\n",
    "default_min_block_size = ctx.target_min_block_size / 1024**2\n",
    "default_max_block_size = ctx.target_max_block_size / 1024**2\n",
    "\n",
    "print(f\"Default min block size: {default_min_block_size:.2f} MiB\")\n",
    "print(f\"Default max block size: {default_max_block_size:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the block size range, configure the `target_min_block_size` and `target_max_block_size` attributes of `DataContext`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.target_min_block_size = 1 * 1024**2\n",
    "ctx.target_max_block_size = 33 * 1024**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-create the dataset and materialize it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.read_parquet(\"/mnt/cluster_storage/data.parquet\")\n",
    "ds_materialized = ds.materialize()\n",
    "ds_materialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how instead of a single block of 64 MiB, we now have 2 blocks of ~32 MiB each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xdel ds\n",
    "%xdel ds_materialized\n",
    "gc.collect()\n",
    "!ray list objects # check objects are garbage collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray memory model\n",
    "\n",
    "Ray manages memory in several ways to efficiently handle distributed tasks:\n",
    "\n",
    "1. **Task Execution Memory**:\n",
    "   - Used by workers to execute tasks and actors.\n",
    "   - Allocated from the worker's heap memory.\n",
    "   - High memory pressure can cause Ray to terminate some tasks to free up resources.\n",
    "\n",
    "2. **Object Store Memory**:\n",
    "   - Serves as the medium for passing data between tasks.\n",
    "   - Objects are stored in a shared memory space, using up to 30% of a node's memory.\n",
    "   - If more space is needed, objects can be spilled to disk or stored on disk in a slower-access format.\n",
    "\n",
    "This setup allows Ray to optimize performance and resource usage in distributed applications.\n",
    "\n",
    "Here is a diagram of the above memory model:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/memory.svg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a numpy array in the object store\n",
    "\n",
    "We will show two tasks:\n",
    "- `producer_task`: creates a numpy array of size 4 GiB\n",
    "- `consumer_task`: reads the output of `producer_task`\n",
    "\n",
    "Here is the code\n",
    "```python\n",
    "@ray.remote\n",
    "def producer_task(size_mb: int = 4 * 1024) -> np.ndarray:\n",
    "    array = np.random.rand((1024**2 * size_mb // 8)).astype(np.float64)\n",
    "    return array\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def consumer_task(array: np.ndarray) -> None:\n",
    "    assert isinstance(array, np.ndarray)\n",
    "    assert not array.flags.owndata\n",
    "\n",
    "arr_ref = producer_task.remote()  # Produce a 4 GiB array.\n",
    "output_ref = consumer_task.remote(arr_ref)  # Consume the array.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens under the hood?\n",
    "- `producer_task` will:\n",
    "    - allocate memory on the worker heap to create the array\n",
    "    - allocate memory on the object store to store the array effectively calling `ray.put`\n",
    "- `consumer_task` will:\n",
    "    - directly access the array in the object store (zero-copy deserialization)\n",
    "    - only incur the cost of copying the array if it is running on a different node.\n",
    "\n",
    "Here is the diagram showing how the object store is used:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/producer-consumer-object-store-v2.png\" width=\"600\">\n",
    "\n",
    "Let's run a script that will inspect the memory usage of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/memory_model/memory_inspection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Data and the Object Store\n",
    "\n",
    "In Ray Data, the object store is used as follows:\n",
    "\n",
    "- Upstream operator tasks place pyarrow Tables in the shared memory object store (similar to `producer_task` in the example above)\n",
    "- Downstream operator tasks \"get\" them from the object store (similar to `consumer_task` in the example above)\n",
    "\n",
    "To verify that a pyarrow Table's underlying array is also zero-copy deserialized, see the below script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/memory_model/pyarrow_zero_copy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Data Resource Management and Limits\n",
    "\n",
    "Ray Data uses a `ResourceManager` to manage resource limits for dataset execution. \n",
    "\n",
    "#### Default Behavior\n",
    "- **Object Store Capacity**: By default, the `ResourceManager` limits usage to 50% of the object store capacity. This is controlled by `ResourceManager.DEFAULT_OBJECT_STORE_MEMORY_LIMIT_FRACTION`.\n",
    "- **CPU and GPU Resources**: The `ResourceManager` will utilize all available CPU and GPU resources on the cluster by default.\n",
    "\n",
    "#### Customizing Resource Limits\n",
    "You can customize resource limits using the `DataContext`:\n",
    "\n",
    "- **Object Store Memory Limit**:\n",
    "  ```python\n",
    "  ctx = ray.data.DataContext.get_current()\n",
    "  ctx.execution_options.resource_limits.object_store_memory = 10e9  # 10 GiB\n",
    "  ```\n",
    "\n",
    "- **CPU and GPU Limits**:\n",
    "  ```python\n",
    "  ctx = ray.data.DataContext.get_current()\n",
    "  ctx.execution_options.resource_limits.cpu = 10\n",
    "  ctx.execution_options.resource_limits.gpu = 5\n",
    "  ```\n",
    "\n",
    "- **Excluding Resources**:\n",
    "  ```python\n",
    "  ctx = ray.data.DataContext.get_current()\n",
    "  ctx.execution_options.exclude_resources = ExecutionResources(cpu=10, gpu=5)  # Exclude 10 CPUs and 5 GPUs\n",
    "  ```\n",
    "\n",
    "To verify that limiting compute resources affects dataset execution, run the below script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/deliberate_backpressure/compute_throttled.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operators and planning\n",
    "\n",
    "We start with a user-defined function (UDF) that will be applied to the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_column(batch, column_name):\n",
    "    batch[column_name] = batch[column_name] + 1\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the UDF to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (\n",
    "    ray.data.read_parquet(\"/mnt/cluster_storage/data.parquet\")\n",
    "    .map_batches(increment_column, fn_kwargs={\"column_name\": \"a\"})\n",
    ")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning\n",
    "\n",
    "Below are the steps that Ray Data takes to plan the execution of a dataset.\n",
    "\n",
    "1. Ray Data optimizes the logical plan performing a series of optimizations to produce an optimized logical plan. \n",
    "\n",
    "2. The plan then gets translated into a physical plan using a physical planner\n",
    "\n",
    "3. The physical plan is then further optimized (e.g. fusing operators) to produce an optimized physical plan.\n",
    "\n",
    "See the below diagram for the planning process:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/get_execution_plan.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logical plan\n",
    "\n",
    "To access the logical plan, we can use the `_plan` attribute of the dataset.\n",
    "\n",
    "`_plan` returns an `ExecutionPlan` which wraps a `LogicalPlan`.\n",
    "\n",
    "The `LogicalPlan` simply represents \"what\" operations will be applied to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logical_plan = ds._plan._logical_plan\n",
    "logical_plan.dag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the optimized physical plan, we can use the `get_execution_plan` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data._internal.logical.optimizers import get_execution_plan\n",
    "\n",
    "optimized_physical_plan = get_execution_plan(ds._plan._logical_plan)\n",
    "optimized_physical_plan.dag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operators\n",
    "\n",
    "The above DAG shows that \"logically\" we require operations like `ReadFiles` and `MapBatches` to be run.\n",
    "\n",
    "Physical plans indicate \"how\" the logical operators will be executed.\n",
    "\n",
    "The above DAG shows a plan of physical operators that will be executed:\n",
    "- `InputDataBuffer` is a placeholder for the input data.\n",
    "- `TaskPoolMapOperator` is a physical operator which will execute logical operations like `ReadFiles` and `MapBatches` using a \"pool\" of Ray Tasks. \n",
    "- The syntax is `{PhysicalOperator}[{LogicalOperator}]`. \n",
    "\n",
    "#### Operator Fusion\n",
    "Under certain conditions, Ray Data will fuse operators together to reduce data movement and improve execution efficiency.\n",
    "\n",
    "Here is the syntax for fusing operators:\n",
    "- `{PhysicalOperator}[{LogicalOperator1}->{LogicalOperator2}]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_repeated = ds.map_batches(increment_column, fn_kwargs={\"column_name\": \"a\"})\n",
    "get_execution_plan(ds_repeated._plan._logical_plan).dag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditions for Operator Fusion\n",
    "\n",
    "Ray Data currently supports fusing two operators if the following are all true:\n",
    "- We are fusing either `MapOperator -> MapOperator` or `MapOperator -> AllToAllOperator`.\n",
    "- They either use the same compute configuration, or the upstream operator uses a task pool while the downstream operator uses an actor pool.\n",
    "- If both operators involve callable classes, the callable classes are the same class AND constructor args are the same for both.\n",
    "- They have compatible remote arguments.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Here are the implementation details for the OperatorFusionRule</summary>\n",
    "\n",
    "You can see the conditions for operator fusion in the `OperatorFusionRule.can_fuse` method.\n",
    "\n",
    "```python\n",
    "from ray.data._internal.logical.rules.operator_fusion import OperatorFusionRule\n",
    "\n",
    "%psource OperatorFusionRule._can_fuse\n",
    "```\n",
    "\n",
    "The `OperatorFusionRule._get_fused_map_operator` method will create a new `MapOperator` with a \"fused\" `MapTransformer`.\n",
    "\n",
    "You can view the implementation of the method below.\n",
    "\n",
    "```python\n",
    "%psource OperatorFusionRule._get_fused_map_operator\n",
    "```\n",
    "\n",
    "Below is the snippet of the code that creates the new/fused `MapOperator`.\n",
    "\n",
    "```python\n",
    "op = MapOperator.create(\n",
    "    up_op.get_map_transformer().fuse(down_op.get_map_transformer()),\n",
    "    input_op,\n",
    "    target_max_block_size=target_max_block_size,\n",
    "    name=name,\n",
    "    compute_strategy=compute,\n",
    "    min_rows_per_bundle=min_rows_per_bundled_input,\n",
    "    ray_remote_args=ray_remote_args,\n",
    "    ray_remote_args_fn=ray_remote_args_fn,\n",
    ")\n",
    "```\n",
    "\n",
    "Let's look at the implementation of `MapTransformer.fuse`\n",
    "\n",
    "\n",
    "```python\n",
    "from ray.data._internal.execution.operators.map_transformer import MapTransformer\n",
    "\n",
    "%psource MapTransformer.fuse\n",
    "```\n",
    "\n",
    "It creates a single `MapTransformer` with a fused init function and transform functions.\n",
    "```python\n",
    "def fused_init_fn():\n",
    "    self_init_fn()\n",
    "    other_init_fn()\n",
    "\n",
    "fused_transform_fns = self._transform_fns + other._transform_fns\n",
    "transformer = MapTransformer(fused_transform_fns, init_fn=fused_init_fn)\n",
    "```\n",
    "\n",
    "Here is how the transformation is then applied.\n",
    "\n",
    "```python\n",
    "%psource MapTransformer.apply_transform\n",
    "```\n",
    "You can see the transform functions are applied sequentially to the input iterable within a single task.\n",
    "\n",
    "This means data is not transferred across the object store anymore.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Topology\n",
    "\n",
    "After constructing the optimized DAG of physical operators, the execution plan is handed to the `StreamingExecutor` to execute.\n",
    "\n",
    "The first step is to build a streaming topology, here is a sample diagram of the streaming topology:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/build_streaming_topology.png\" width=\"1000\">\n",
    "\n",
    "Each physical operator will be wired to the next physical operator downstream in the execution plan. \n",
    "\n",
    "An upstream operator's external output queue will *refer to the same queue* as the input of the downstream operator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data flow within an operator\n",
    "\n",
    "Below is a diagram showing the data flow within an operator.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/data_flow_simplified_v4.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference bundles are passed between operators, where:\n",
    "- Each reference bundle packages references to blocks and metadata about the blocks.\n",
    "- Most operators will process bundles by submitting tasks.\n",
    "\n",
    "Processing a bundle means the following steps:\n",
    "1. Consume the block reference from an external queue\n",
    "2. Add it to an internal queue for processing\n",
    "3. Submit a task to process the block.\n",
    "4. Generate blocks from the task \n",
    "5. Move the blocks to an internal out-queue\n",
    "6. Send the blocks downstream via the external queue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-to-one operator mapping from block to task\n",
    "\n",
    "In the case of One-to-One operators:\n",
    "- Each reference bundle contains a single block \n",
    "- Each task will process a single block.\n",
    "\n",
    "To see how this works, run the below script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/data_flow/one_to_one_operator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a screenshot from the Ray Dashboard Job UI showing the Ray Data overview of the two datasets created by the script.\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/ray-data-overview-one-to-one-op.png\" width=\"900\">\n",
    "\n",
    "Below is a screenshot from the Ray Dashboard Job UI showing the Ray Core overview of the two datasets created by the script.\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/ray-core-overview-one-to-one-op.png\" width=\"900\">\n",
    "\n",
    "We can see that:\n",
    "- ds-with-1-block only generated 1 block\n",
    "- ds-with-1-block took a total of 8s to execute `_slow_add`\n",
    "\n",
    "Whereas:\n",
    "- ds-with-8-blocks generated 8 blocks\n",
    "- ds-with-8-blocks took a total of 1s to execute `_slow_add`\n",
    "\n",
    "This shows that the more blocks we have, the more tasks we can submit and in this case, the faster the dataset will be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Data Dashboard\n",
    "\n",
    "The above dataflow is measured and visualized under the Metrics tab of the Ray Data Dashboard, specifically the following sections\n",
    "- Ray Data Metrics (Overview)\n",
    "- Ray Data Metrics (Inputs)\n",
    "- Ray Data Metrics (Outputs)\n",
    "- Ray Data Metrics (Tasks)\n",
    "- Ray Data Metrics (Object Store Memory)\n",
    "- Ray Data Metrics (Iteration)\n",
    "\n",
    "\n",
    "Here is the same dataflow diagram above with hooks shown for both input and output metrics.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/pinterest/data_flow_simplified_v4_input_output_metrics.png\" width=\"800\">\n",
    "\n",
    "\n",
    "Here is the same diagram with the object store metrics included.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/pinterest/data_flow_simplified_v4_object_store_metrics.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming executor's scheduling loop\n",
    "\n",
    "The `StreamingExecutor` schedules operators to run and moves data between them.\n",
    "\n",
    "The executor and operators are located on the process where dataset execution starts. \n",
    "\n",
    "* For batch inference jobs, this process is usually the driver. \n",
    "* For training jobs, the executor runs on a special actor called `SplitCoordinator` which handles `streaming_split()`.\n",
    "\n",
    "Tasks and actors launched by operators are scheduled across the cluster, and outputs are stored in Ray's distributed object store. The executor manipulates references to objects, and doesn't fetch the underlying data itself to the executor.\n",
    "\n",
    "### The Scheduling Loop\n",
    "\n",
    "The `StreamingExecutor` takes an event-loop like approach to scheduling. \n",
    "\n",
    "Each step of the loop works like this:\n",
    "\n",
    "1. Wait until running tasks and actors have new outputs (up to a 0.1s timeout)\n",
    "2. Move new outputs from the streaming generator buffer into the appropriate operator out-queues.\n",
    "3. Choose some operators and assign new inputs to them. (Operators process the new inputs either by launching new tasks or manipulating metadata.)\n",
    "4. Repeat until all operators are completed.\n",
    " \n",
    "Here is a diagram of the executor loop:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/scheduling_loop_v3.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Here is where to find the scheduling loop code:</summary>\n",
    "\n",
    "```python\n",
    "from ray.data._internal.execution.streaming_executor import StreamingExecutor\n",
    "\n",
    "%psource StreamingExecutor._scheduling_loop_step\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operator Selection\n",
    "\n",
    "Choosing the best operator to assign inputs is one of the most important decisions in Ray Data. \n",
    "\n",
    "The executor can schedule an operator if the operator satisfies the following conditions:\n",
    "\n",
    "- The operator has inputs.\n",
    "- There are adequate resources available.\n",
    "- The operator isn't backpressured.\n",
    "\n",
    "If there are multiple viable operators, the executor chooses the operator with the **smallest out queue in terms of MB produced onto the object store**.\n",
    "\n",
    "Intuitively, this means that the \"slowest producer will be chosen to avoid bottlenecks\".\n",
    "\n",
    "Here is a diagram of the operator selection process:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/selecting_an_operator.png\" width=1000>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Here are the technical details of the operator selection process:</summary>\n",
    "\n",
    "`scheduling_loop_step()` will invoke:\n",
    "  \n",
    "- `select_operator_to_run()` to choose an operator to execute.\n",
    "  - `select_operator_to_run()` will check if the operator is submission-backpressured by:\n",
    "    - Checking the operator's resource usage\n",
    "    - Checking the operator's backpressure policies\n",
    "  - `select_operator_to_run()` will then choose a non-backpressured operator with ready inputs and the smallest out-queue size.\n",
    "\n",
    "Here is the code for `select_operator_to_run()`:\n",
    "\n",
    "```python\n",
    "from ray.data._internal.execution.streaming_executor_state import select_operator_to_run\n",
    "\n",
    "%psource select_operator_to_run\n",
    "```\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource management and allocation\n",
    "\n",
    "Ray data will:\n",
    "- dynamically allocate resources across operators\n",
    "- backpressure operators that have exceeded their resource budgets\n",
    "\n",
    "### Operator backpressure in Ray Data\n",
    "\n",
    "Backpressure in Ray Data is essential for managing resource contention and ensuring fair resource utilization across operators. Its primary goal is to maximize execution plan throughput by controlling task flow.\n",
    "\n",
    "### How Backpressure Works\n",
    "\n",
    "Backpressure can be applied to each physical operator in an execution plan through two approaches:\n",
    "\n",
    "1. **Submission-Based Backpressure**: Prevents an operator from submitting new tasks if it exceeds its resource budget.\n",
    "2. **Output-Based Backpressure**: Prevents an operator from moving outputs to its out-queue if it produces more data than its resource budget allows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is backpressure applied?\n",
    "\n",
    "Below is a diagram of the scheduling loop highlighting where backpressure is applied.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/scheduling_loop_with_backpressure.png\" width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Here is how backpressure is implemented:</summary>\n",
    "\n",
    "##### Submission-based backpressure\n",
    "Submission-based backpressure is implemented in the `ResourceManager.can_submit_new_task()` method. It is used in the `select_operator_to_run()` method to determine if an operator can submit new tasks.\n",
    "\n",
    "##### Output-based backpressure\n",
    "\n",
    "Output-based backpressure is implemented in the `ResourceManager.max_task_output_bytes_to_read()` method. It is used in the `process_completed_tasks()` method to determine the maximum bytes of task outputs that can be read and moved to an external queue.\n",
    "\n",
    "<details>\n",
    "<summary>Here are the technical details where output-based backpressure is applied:</summary>\n",
    "\n",
    "The `scheduling_loop_step()` will invoke:\n",
    "\n",
    "- `process_completed_tasks()` to:\n",
    "  - Wait and gather completed task outputs from all operators\n",
    "  - Move completed task outputs to the operator's in-queue\n",
    "\n",
    "`process_completed_tasks` will make use of `max_task_output_bytes_to_read()` to determine the maximum bytes of task outputs that can be read and moved to an external queue.\n",
    "\n",
    "Here is the code for `process_completed_tasks()`:\n",
    "\n",
    "```python\n",
    "from ray.data._internal.execution.streaming_executor_state import process_completed_tasks\n",
    "\n",
    "%psource process_completed_tasks\n",
    "```\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Backpressure\n",
    "\n",
    "A component of the `StreamingExecutor` is the `ResourceManager`.\n",
    "\n",
    "The `ResourceManager` is responsible for setting the resource budgets for each operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Operator Resource Budgets\n",
    "\n",
    "It is generally a useful practice to report debug logs from the `ResourceManager` to help you understand resource budgetting and allocation.\n",
    "\n",
    "You can do so by setting `RAY_DATA_DEBUG_RESOURCE_MANAGER=1` in your environment.\n",
    "\n",
    "See the below script for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/resource_manager/debug.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Allocation\n",
    "\n",
    "#### Tracking Operator Usage\n",
    "A `ResourceManager` tracks the usage of every operator \n",
    "\n",
    "Each Usage estimate contains:\n",
    "- CPU\n",
    "- GPU\n",
    "- Object store memory\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Here is how the ResourceManager tracks operator usage:</summary>\n",
    "\n",
    "The `ResourceManager` tracks operator usage by calling `update_usages()`.\n",
    "\n",
    "`ResourceManager.update_usages()` is called as part of the scheduling loop:\n",
    "1. At the very beginning of the loop\n",
    "2. After waiting on running task outputs\n",
    "3. Before selecting an operator to execute\n",
    "\n",
    "```python\n",
    "from ray.data._internal.execution.resource_manager import ResourceManager\n",
    "\n",
    "%psource ResourceManager.update_usages\n",
    "```\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Budget Allocation\n",
    "\n",
    "**Intuition:**\n",
    "The rate at which source tasks are scheduled, has to be just right.\n",
    "- If the *source* tasks are launched too slowly, the downstream operators will have no input to process, leading to under-utilization of resources. \n",
    "- Conversely, if the source tasks are launched too aggressively, then these tasks will occupy most resources, reducing available resources for downstream operators and slowing down execution.\n",
    "    - It will eventually cause back-pressuring of the source operator \n",
    "    - It might also force intermediate data to be spilled to disk, \n",
    "\n",
    "\n",
    "Ray data uses a dynamic budget algorithm to regulate the rate at which source tasks are launched. \n",
    "- Intuitively, the budget is an optimistic estimate of the resources available for new data partitions to enter the system.\n",
    "- The key idea is to equalize the processing rates of each operator, in terms of bytes per second, in order to maximize the overall throughput. \n",
    "\n",
    "\n",
    "Ray Data implements dynamic budget allocation using a reservation-based approach where by default, 50% of the global resources are reserved for operator outputs, while the remaining resources are shared among all operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Here are more details on the reservation-based approach:</summary>\n",
    "\n",
    "1. Reservation-based resource allocation at the operator level via the `ReservationOpResourceAllocator`\n",
    "\n",
    "\n",
    "### Reservation-based resource allocation at the operator level\n",
    "\n",
    "Overall, this approach allows for resource sharing across operators, while still allowing for operator-specific resource reservations.\n",
    "\n",
    "It works in the following way:\n",
    "1. For each map operator, resources are reserved as `reservation_ratio * global_resources / num_map_ops`, with half reserved specifically for operator outputs, excluding pending task outputs.\n",
    "2. Non-reserved resources are shared among all operators.\n",
    "3. In each scheduling iteration, each map operator receives \"remaining of their own reserved resources\" plus \"remaining of shared resources / num_map_ops\" resources.\n",
    "\n",
    "The `reservation_ratio` is set to 50% by default. Users can tune this value to adjust how aggressive or conservative the resource allocation is. A higher value will make the resource allocation more even, but may lead to underutilization. And vice versa.\n",
    "\n",
    "<details>\n",
    "<summary>Here is where to find the code for the `ReservationOpResourceAllocator`</summary>\n",
    "\n",
    "```python\n",
    "from ray.data._internal.execution.resource_manager import ReservationOpResourceAllocator\n",
    "\n",
    "%psource ReservationOpResourceAllocator\n",
    "```\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
